# GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents (ACL2025)

This repository is the official implementation of **Benchmarking Domain-Oriented Guideline Following for LLM Agents.** [Our Paper](https://arxiv.org/abs/2505.11368).

## âš™ï¸ Requirements

To install requirements:

```
pip install -r requirements.txt

```

## ğŸ“Â Data

All data of GuideBench is placed in `./data` . Different category tasks of the dataset is organized in the format of a list respectively, where each element of the list is an instance of the dataset. The format of an instance is as follows.

- `Instruction` (string):  the overarching task objective.
- `Guidelines` (string): a set of domain-specific rules that inform the task structure.
    - `type` (string): the type of the guideline rule
    - `rule_id` (string): the index of the guideline rule
    - `rule_text` (string): the actual text of the guideline rule
- `Context` (string): a relevant text passage
- `Groundtruth` (string): the human-checked ground truth, which are designed to verify the correctness of the answers generated by LLMs.
    - `ReferenceAnswer` (integer or string): the 0/1 or numbers or A/B/C/D corresponding to QA tasks and multiple-choice tasks.
    - `ReferenceAnalysis` (string): the reference analysis for why the reference answer is correct.

Here is an example of GuideBench.

```json
{
    "Instruction": "ä¼˜æƒ åˆ¸æ•°å­¦è®¡ç®—é¢˜ç›®",
    "Guidelines": [
      {
        "type": "æŠ˜æ‰£ä¼˜æƒ åˆ¸",
        "rule_id": "rule_2",
        "rule_text": "æŠ˜æ‰£ä¼˜æƒ åˆ¸ï¼š7 æŠ˜ä¼˜æƒ åˆ¸ï¼Œé€‚ç”¨äºé™¤è¿åŠ¨å™¨æå¤–çš„æ‰€æœ‰å®¶å±…ç”¨å“ç±»å•†å“ï¼Œæ¯ä¸ªè®¢å•ä»…é™ä½¿ç”¨ä¸€æ¬¡ï¼Œå¯ä¸æ»¡å‡ä¼˜æƒ åˆ¸ã€å›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸å åŠ ä½¿ç”¨"
      },
      {
        "type": "å›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸",
        "rule_id": "rule_11",
        "rule_text": "å›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸ï¼š25 å…ƒä¼˜æƒ åˆ¸ï¼Œæ— æ¶ˆè´¹é—¨æ§›ï¼Œå¯ç”¨äºè´­ä¹°å¹³å°å†…é™¤é£Ÿå“é¥®æ–™ç±»å’Œç¾å¦†ç±»å¤–çš„æ‰€æœ‰å•†å“ï¼Œä½†æ¯ä¸ªè®¢å•ä»…é™ä½¿ç”¨ä¸€æ¬¡ï¼Œå¯ä¸æŠ˜æ‰£ä¼˜æƒ åˆ¸å åŠ ä½¿ç”¨ï¼Œä¸å¯ä¸æ»¡å‡ä¼˜æƒ åˆ¸å åŠ ä½¿ç”¨"
      },
      {
        "type": "æ»¡å‡ä¼˜æƒ åˆ¸",
        "rule_id": "rule_15",
        "rule_text": "æ»¡å‡ä¼˜æƒ åˆ¸ï¼šæ»¡ 700 å‡ 180ï¼Œå¯ä¸å›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸å åŠ ä½¿ç”¨ï¼Œä½†ä¸å¯ä¸æŠ˜æ‰£ä¼˜æƒ åˆ¸å åŠ ï¼Œæ¯ä¸ªè®¢å•ä»…é™ä½¿ç”¨ä¸€æ¬¡ï¼Œé€‚ç”¨äºå¹³å°å†…é™¤é£Ÿå“é¥®æ–™ç±»ã€ç”µå­äº§å“ç±»å’Œè¿åŠ¨å™¨æç±»å¤–çš„æ‰€æœ‰å•†å“"
      },
      {
        "type": "ç»„åˆä¼˜æƒ åˆ¸ä½¿ç”¨é™åˆ¶",
        "rule_id": "rule_20",
        "rule_text": "åŒä¸€è®¢å•ä¸­ï¼Œè‹¥åŒæ—¶ä½¿ç”¨æ»¡å‡ä¼˜æƒ åˆ¸å’Œå›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸ï¼Œéœ€å…ˆè®¡ç®—æ»¡å‡é‡‘é¢ï¼Œå†è®¡ç®—å›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸çš„æŠµæ‰£é‡‘é¢"
      },
      {
        "type": "æŠ˜æ‰£ä¼˜æƒ åˆ¸",
        "rule_id": "rule_8",
        "rule_text": "æŠ˜æ‰£ä¼˜æƒ åˆ¸ï¼š9 æŠ˜ä¼˜æƒ åˆ¸ï¼Œé€‚ç”¨äºå¹³å°å†…é™¤ç¾å¦†ç±»å•†å“å¤–çš„æ‰€æœ‰é¥°å“ç±»å•†å“ï¼Œæ¯ä¸ªè®¢å•ä»…é™ä½¿ç”¨ä¸€æ¬¡ï¼Œä¸å¯ä¸æ»¡å‡ä¼˜æƒ åˆ¸ã€å›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸å åŠ ä½¿ç”¨"
      },
      {
        "type": "æ»¡å‡ä¼˜æƒ åˆ¸",
        "rule_id": "rule_44",
        "rule_text": "æ»¡å‡ä¼˜æƒ åˆ¸ï¼šæ–°å¢æ»¡ 350 å‡ 90ï¼Œå¯ä¸æŠ˜æ‰£ä¼˜æƒ åˆ¸å åŠ ä½¿ç”¨ï¼Œä¸å¯ä¸å›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸å åŠ ï¼Œæ¯ä¸ªè®¢å•ä»…é™ä½¿ç”¨ä¸€æ¬¡ï¼Œæ— é€‚ç”¨æ—¶é—´é™åˆ¶ï¼Œé€‚ç”¨äºå¹³å°å†…ç©å…·ç±»å•†å“"
      }
    ],
    "Context": "åœ¨æŸç”µå•†å¹³å°è´­ç‰©ï¼Œå°å¼ å‡†å¤‡è´­ä¹°ä¸€äº›å®¶å±…ç”¨å“ï¼ˆéè¿åŠ¨å™¨æï¼‰ã€ä¸€æ‰¹é¥°å“ï¼ˆéç¾å¦†ç±»ï¼‰ä»¥åŠä¸€äº›ç©å…·ã€‚å®¶å±…ç”¨å“æ€»ä»·ä¸º 400 å…ƒï¼Œé¥°å“æ€»ä»·ä¸º 150 å…ƒï¼Œç©å…·æ€»ä»·ä¸º 380 å…ƒã€‚é‚£ä¹ˆå°å¼ è´­ä¹°è¿™äº›å•†å“ï¼Œæœ€å°‘éœ€è¦æ”¯ä»˜å¤šå°‘é’±ï¼Ÿ",
    "Groundtruth": {
      "ReferenceAnswer": "695",
      "ReferenceAnalysis": "é¦–å…ˆè®¡ç®—å®¶å±…ç”¨å“ä½¿ç”¨7æŠ˜ä¼˜æƒ åˆ¸åçš„ä»·æ ¼ä¸º400Ã—0.7=280å…ƒã€‚é¥°å“ä½¿ç”¨9æŠ˜ä¼˜æƒ åˆ¸åçš„ä»·æ ¼ä¸º150Ã—0.9 =135å…ƒï¼Œæ­¤æ—¶æ€»è®¡280+135+380=795å…ƒã€‚è‹¥ä¸ä½¿ç”¨9æŠ˜åŠµï¼Œç©å…·æ€»ä»·380å…ƒï¼Œå¯ä½¿ç”¨æ»¡350å‡90çš„æ»¡å‡ä¼˜æƒ åˆ¸ï¼Œç©å…·å®é™…éœ€æ”¯ä»˜380-90=290å…ƒï¼Œæ­¤æ—¶å•†å“æ€»ä»·ä¸º280+150+290=720å…ƒï¼Œä½†æ­¤æ—¶æ— æ³•ä½¿ç”¨700æ»¡å‡åˆ¸ï¼Œå†ä½¿ç”¨25å…ƒå›ºå®šé‡‘é¢ä¼˜æƒ åˆ¸ï¼Œæœ€ç»ˆéœ€æ”¯ä»˜720 - 25 = 695å…ƒã€‚"
    }
}
```

## ğŸ’¡ Evaluation

### Step1: Generating the responses

First, you need to deploy your target LLM and generate responses of it. The prompt template is in `generate_resp.py` , for both API-based models and open-sourced ones.

We suggest using greedy decoding to avoid the randomness of decoding.

### Step2: Evaluation

Then, you can evaluate any desired model via `evaluate.py` to generate a text file that consists of the modelâ€™s accuracy. Just fill in with YOUR_ANSWER_PATH.

## ğŸ‘ Citation

```
@misc{diao2025guidebenchbenchmarkingdomainorientedguideline,
      title={GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents}, 
      author={Lingxiao Diao and Xinyue Xu and Wanxuan Sun and Cheng Yang and Zhuosheng Zhang},
      year={2025},
      eprint={2505.11368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.11368}, 
}
```

Please kindly cite our paper if this paper and the codes are helpful.
